{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ad7a3c",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db7c2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n",
      "현재 가상 환경 경로: c:\\working\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\n",
      "c:\\working\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(\"현재 가상 환경 경로:\", sys.prefix)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da27da7",
   "metadata": {},
   "source": [
    "## precession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f1f97e",
   "metadata": {},
   "source": [
    "### package setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e197ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\working\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\working\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\albumentations\\__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as albu\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "import shutil\n",
    "import os\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66b0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f82464",
   "metadata": {},
   "source": [
    "### definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb617aa",
   "metadata": {},
   "source": [
    "#### 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46afb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_dir(target_path):\n",
    "    if os.path.exists(target_path):\n",
    "        shutil.rmtree(target_path)\n",
    "        os.makedirs(target_path)\n",
    "    else:\n",
    "        os.makedirs(target_path)\n",
    "def build_dir(target_path):\n",
    "    if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d634ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropping_image(df, save_path):\n",
    "    print(\"전체 사진 수: \", len(df))\n",
    "    size = 512\n",
    "    for idx in (df.index):\n",
    "        base_names = df.loc[idx,'base_names']    \n",
    "        img_path = df.loc[idx,'file_dir']\n",
    "\n",
    "        img_rgb=cv2.imread(img_path, cv2.IMREAD_COLOR)  # 원본이미지\n",
    "        max_height,max_width = img_rgb.shape[0],img_rgb.shape[1]\n",
    "        \n",
    "        num=0\n",
    "        \n",
    "        label_dir = df.loc[idx,'autolabeling_dir']\n",
    "        result_dir = df.loc[idx,'img_dir']\n",
    "\n",
    "        rebuild_dir(label_dir)    \n",
    "        build_dir(result_dir)      \n",
    "\n",
    "        for height in tqdm(range(0, img_rgb.shape[0], size)):\n",
    "            for width in range(0, img_rgb.shape[1], size):\n",
    "\n",
    "                img_rgb_crop = img_rgb[height:height+size, width:width+size, :]  # 512*512 절삭(높이, 너비)\n",
    "                if height+size > max_height or width+size > max_width:\n",
    "                    pass\n",
    "                else:\n",
    "                    name = str(base_names)+'_Cropped_'+'%05d.png' % num\n",
    "                    tile_dir = os.path.join(save_path, name)\n",
    "                    cv2.imwrite(tile_dir,img_rgb_crop)\n",
    "                    num += 1\n",
    "                    \n",
    "        print(f\"파일명: {df.loc[idx,'file_name']}\\nSaved cropped image: {num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0ab7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(BaseDataset):\n",
    "    CLASSES = ['x' for x in range(255)]\n",
    "    CLASSES.append('cell')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        images_dir,\n",
    "        images_files,\n",
    "        classes=None,\n",
    "        preprocessing=None,\n",
    "    ):\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in (images_files)]  # image_path\n",
    "        self.preprocessing = preprocessing\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "\n",
    "        self.tile_size = 512\n",
    "\n",
    "        # 전체 이미지에서 모든 타일 정보를 미리 생성\n",
    "        self.tiles_info = []\n",
    "        for img_idx, image_path in enumerate(self.images_fps):\n",
    "            image = cv2.imread(image_path)\n",
    "            height, width, _ = image.shape\n",
    "            \n",
    "            for h in range(0, height, self.tile_size):\n",
    "                for w in range(0, width, self.tile_size):\n",
    "                    self.tiles_info.append({\n",
    "                        'img_idx': img_idx, # 원본 이미지 인덱스\n",
    "                        'h_start': h,\n",
    "                        'w_start': w,\n",
    "                    })\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # i번째 타일 정보 가져오기\n",
    "        tile_info = self.tiles_info[i]\n",
    "        img_idx = tile_info['img_idx']\n",
    "        h_start = tile_info['h_start']\n",
    "        w_start = tile_info['w_start']\n",
    "        \n",
    "        # 원본 이미지 로드\n",
    "        image_path = self.images_fps[img_idx]\n",
    "        image = cv2.imread(image_path)\n",
    "        # 흑백일 때(2D array) BGR로 바꿔 주기\n",
    "        if image.ndim == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "        else:\n",
    "            # 컬러 파일이라면 기본 BGR 로드 → RGB 로 변환\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        \n",
    "        # 타일 크롭\n",
    "        tile = image[h_start:h_start+self.tile_size, w_start:w_start+self.tile_size]\n",
    "        \n",
    "        # 전처리 적용\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=tile)\n",
    "            tile = sample['image']\n",
    "\n",
    "        # 원본 이미지의 이름과 크기 정보 반환\n",
    "        original_image_name = os.path.splitext(os.path.basename(image_path))[0]    \n",
    "        original_height, original_width, _ = image.shape\n",
    "        \n",
    "        return tile, h_start, w_start, original_height, original_width, original_image_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tiles_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "549ab419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    if x.ndim == 2:  # 채널이 없는 경우    \n",
    "        x = np.expand_dims(x, axis=-1)  # (H, W) → (H, W, 1)\n",
    "    if x.shape[2] == 1:  # 1채널일 경우 → 3채널로 반복\n",
    "        x = np.repeat(x, 3, axis=2)\n",
    "    x = x.transpose(2, 0, 1).astype('float32')  # (C, H, W)\n",
    "    return torch.from_numpy(x)\n",
    "\n",
    "def get_preprocessing(preprocessing_fn):\n",
    "\n",
    "    _transform = [\n",
    "        albu.PadIfNeeded(512, 512, always_apply=True, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "        albu.Lambda(image=preprocessing_fn),\n",
    "        albu.Lambda(image=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19c3a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: [(tile, h, w, H, W, name),  … ]  형태의 리스트\n",
    "    -> ([tiles…], [h_starts…], [w_starts…], [heights…], [widths…], [names…])\n",
    "    \"\"\"\n",
    "    tiles, h_starts, w_starts, heights, widths, names = zip(*batch)\n",
    "    tiles_batch = torch.stack(tiles)\n",
    "    # tiles: tuple of tensors, h_starts 등은 tuple of int, names는 tuple of str\n",
    "    return (tiles_batch), list(h_starts), list(w_starts), list(heights), list(widths), list(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c6d85",
   "metadata": {},
   "source": [
    "#### mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1543253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_less_white_pixels(image_path, threshold = 1):\n",
    "    \"\"\"\n",
    "    이미지 내에 지정된 threshold 값 미만의 흰색 픽셀이 있는지 확인합니다.\n",
    "    :param image_path: 이미지 파일의 경로\n",
    "    :param threshold: 흰색 픽셀의 최대 수\n",
    "    :return: 지정된 threshold 값을 미만의 흰색 픽셀이 있으면 True, 그렇지 않으면 False\n",
    "    \"\"\"\n",
    "    white_pixel_count = 0\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # 이미지 모드 확인: 'L' 모드는 흑백, 'RGB'는 컬러\n",
    "            if img.mode != 'L':\n",
    "                # 흑백이 아니면 흑백으로 변환하여 픽셀 값 비교를 용이하게 합니다.\n",
    "                img = img.convert('L')\n",
    "            \n",
    "            pixels = img.getdata()\n",
    "            \n",
    "            # 각 픽셀 값이 255(흰색)인지 직접 확인\n",
    "            for pixel in pixels:\n",
    "                if pixel == 255:\n",
    "                    white_pixel_count += 1\n",
    "                if white_pixel_count >= threshold:\n",
    "                    return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"이미지 처리 중 오류 발생: {e}\")\n",
    "        return False # 오류 발생 시 False 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07121728",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blurrd_img(tiff_files):\n",
    "    ## 원본 이미지\n",
    "    img = cv2.imread(tiff_files, cv2.IMREAD_COLOR)\n",
    "\n",
    "    ## 블러 처리\n",
    "    blur = cv2.medianBlur(img, 9)\n",
    "\n",
    "    hsv_img = cv2.cvtColor(blur, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    ## hsv 채널분리\n",
    "    h, s, v = cv2.split(hsv_img)\n",
    "\n",
    "    _, thr_s = cv2.threshold(s, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    __cached__, thr_v = cv2.threshold(v, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    thr_v -= 255\n",
    "\n",
    "    # Contours 찾기\n",
    "    contours_s, _ = cv2.findContours(thr_s, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    contours_v, _ = cv2.findContours(thr_v, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "    contour_img_s = np.zeros_like(thr_s)\n",
    "    cn_filled_s = cv2.drawContours(contour_img_s, contours_s, -1, (255,255,255), -1)\n",
    "\n",
    "    contour_img_v = np.zeros_like(thr_v)\n",
    "    cn_filled_v = cv2.drawContours(contour_img_v, contours_v, -1, (255,255,255), -1)\n",
    "\n",
    "    ## hsv 채널 중 s,v 채널 영역 합침\n",
    "    mask_area = cv2.bitwise_and(cn_filled_s, cn_filled_v)\n",
    "\n",
    "    # Contours 찾기\n",
    "    contours, _ = cv2.findContours(mask_area, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Contours를 그릴 이미지 생성\n",
    "    contour_img = np.zeros_like(mask_area)\n",
    "\n",
    "    # Contours 그리기\n",
    "    cn_filled = cv2.drawContours(contour_img, contours, -1, (255,255,255), -1)\n",
    "\n",
    "    result_img = img.copy()\n",
    "    result_img[cn_filled==0] = [255, 255, 255]  # 흰색으로 채우기\n",
    "\n",
    "    return result_img, mask_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ea6aa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pixels_in_mask(mask):\n",
    "    \"\"\"마스크 안의 픽셀 수를 반환합니다.\"\"\"\n",
    "    return np.sum(mask)\n",
    "\n",
    "def draw_contours_on_image(image, mask, color, full_mask=None):\n",
    "    \"\"\"\n",
    "    Mask의 테두리를 이미지에 그립니다.\n",
    "    (수정) full_mask가 제공되면, full_mask 영역 안에 포함된 contour만 그립니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 예측 마스크(U-Net)에서 모든 contour 찾기\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    image_with_contours = image.copy()\n",
    "    \n",
    "    filtered_contours = []\n",
    "    \n",
    "    if full_mask is not None:\n",
    "        # 2. 필터링 로직: full_mask 영역 안에 있는 contour만 선택\n",
    "        for cnt in contours:\n",
    "            try:\n",
    "                # 3. 예측 contour의 중심점 계산\n",
    "                M = cv2.moments(cnt)\n",
    "                if M['m00'] == 0: continue # 면적이 0인 contour 방지\n",
    "                \n",
    "                cx = int(M['m10'] / M['m00'])\n",
    "                cy = int(M['m01'] / M['m00'])\n",
    "\n",
    "                # 4. 중심점이 full_mask(blurrd_img 결과)의 흰색 영역(255)에 있는지 확인\n",
    "                # (배열 인덱싱은 y, x 순서)\n",
    "                if full_mask[cy, cx] != 0:\n",
    "                    filtered_contours.append(cnt)\n",
    "            except Exception as e:\n",
    "                # 중심점 계산이 불가능한 아주 작은 contour 등 예외 처리\n",
    "                pass \n",
    "    else:\n",
    "        # full_mask가 없으면 모든 contour를 그림\n",
    "        filtered_contours = contours\n",
    "        \n",
    "    # 5. 필터링된 contour만 원본 이미지에 그리기\n",
    "    cv2.drawContours(image_with_contours, filtered_contours, -1, color, 10)\n",
    "    return image_with_contours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6be70b",
   "metadata": {},
   "source": [
    "#### histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94692df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segmentation_mask(mask_path, bin_size=1000):\n",
    "    \"\"\"\n",
    "    저장된 전체 마스크 이미지를 분석하여 Contour 개수와 크기 분포를 반환합니다.\n",
    "\n",
    "    :param mask_path: 분석할 마스크 이미지 파일 경로 (예: '..._mask.png')\n",
    "    :param bin_size: 히스토그램의 구간 크기 (기본값: 1000 픽셀)\n",
    "    :return: (전체 Contour 개수, 히스토그램 카운트, 히스토그램 구간) 튜플\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. 마스크 이미지를 그레이스케일로 로드\n",
    "    mask_image = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    if mask_image is None:\n",
    "        print(f\"오류: 이미지를 로드할 수 없습니다. 경로: {mask_path}\")\n",
    "        return 0, None, None\n",
    "\n",
    "    # 2. Contour 찾기\n",
    "    # cv2.RETR_EXTERNAL: 가장 바깥쪽의 Contour만 찾습니다.\n",
    "    # cv2.CHAIN_APPROX_SIMPLE: Contour의 꼭짓점만 저장하여 메모리를 절약합니다.\n",
    "    contours, _ = cv2.findContours(\n",
    "        mask_image, \n",
    "        cv2.RETR_EXTERNAL, \n",
    "        cv2.CHAIN_APPROX_SIMPLE\n",
    "    )\n",
    "    \n",
    "    total_contours = len(contours)\n",
    "    print(f\"--- 마스크 분석 결과 ('{os.path.basename(mask_path)}') ---\")\n",
    "    print(f\"✅ 전체 Contour 개수: {total_contours} 개\")\n",
    "    \n",
    "    if total_contours == 0:\n",
    "        print(\"분석할 Contour가 없습니다.\")\n",
    "        return 0, None, None\n",
    "\n",
    "    # 3. 각 Contour의 픽셀 면적(Area) 계산\n",
    "    contour_areas = [cv2.contourArea(cnt) for cnt in contours]\n",
    "    \n",
    "    # 4. 1000 단위로 찢어서 히스토그램화\n",
    "    max_area = max(contour_areas)\n",
    "    \n",
    "    # 히스토그램 구간(bins) 설정\n",
    "    # 예: max_area가 3500이면, bins = [0, 1000, 2000, 3000, 4000]\n",
    "    bins = np.arange(0, max_area + bin_size, bin_size)\n",
    "    \n",
    "    # NumPy를 사용하여 히스토그램 계산\n",
    "    hist_counts, bin_edges = np.histogram(contour_areas, bins=bins)\n",
    "    \n",
    "    # # 5. 결과 출력\n",
    "    # print(\"\\n--- Contour 크기 분포 (히스토그램) ---\")\n",
    "    # for i in range((total_contours)):\n",
    "    #     print(f\"{i}번째 contour 크기: {contour_areas[i]} 픽셀\")\n",
    "        \n",
    "    return total_contours, hist_counts, bin_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3dfe896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_skip_empty(hist_counts, bin_edges, save_path ,name):\n",
    "    \"\"\"\n",
    "    계산된 히스토그램 데이터에서 카운트가 0인 구간을 '건너뛰고' (제외하고)\n",
    "    막대그래프로 시각화하고 저장합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    if hist_counts is None or np.sum(hist_counts) == 0:\n",
    "        print(\"시각화할 히스토그램 데이터가 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # --- 수정된 부분: 10이하인 구간 필터링 ---\n",
    "    \n",
    "    # 1. 카운트가 0보다 큰(> 0) 구간의 인덱스를 찾습니다.\n",
    "    non_zero_indices = np.where(hist_counts > 10)[0]\n",
    "    \n",
    "    if len(non_zero_indices) == 0:\n",
    "        print(\"시각화할 데이터가 없습니다 (모든 구간의 카운트가 0입니다).\")\n",
    "        return\n",
    "\n",
    "    # 2. 0이 아닌 카운트만 필터링합니다.\n",
    "    filtered_counts = hist_counts[non_zero_indices]\n",
    "    \n",
    "    # 3. 필터링된 구간에 해당하는 Bin 레이블을 생성합니다.\n",
    "    # 예: \"0-1000\", \"2000-3000\" (1000-2000이 비어있다면 건너뜀)\n",
    "    bin_labels = []\n",
    "    for i in non_zero_indices:\n",
    "        label = f\"{int(bin_edges[i])}-{int(bin_edges[i+1])}\"\n",
    "        bin_labels.append(label)\n",
    "    # --- 여기까지 수정 ---\n",
    "\n",
    "    # 4. 필터링된 데이터로 막대그래프를 생성합니다.\n",
    "    # 그래프의 너비를 데이터 개수에 따라 동적으로 조절\n",
    "    plt.figure(figsize=(max(10, len(bin_labels) * 0.5), 6))\n",
    "    plt.bar(bin_labels, filtered_counts, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    plt.title('Histogram of Contour Areas (10 more section)')\n",
    "    plt.xlabel('Pixel Area Range (px)')\n",
    "    plt.ylabel('Number of Contours (Count)')\n",
    "    # plt.xticks(rotation=45, ha='right') # 레이블이 겹치지 않게 회전\n",
    "    plt.tight_layout() # 레이아웃 최적화\n",
    "    \n",
    "    # 5. 그래프를 이미지 파일로 저장\n",
    "    plt.savefig(os.path.join(save_path,f'{name}_histogram.png'))\n",
    "    plt.show()\n",
    "    print(f\"\\n✅ (카운트가 10미만인 구간 제외) 히스토그램 그래프가 '{save_path}'에 저장되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b7224",
   "metadata": {},
   "source": [
    "## main test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d1a78",
   "metadata": {},
   "source": [
    "### path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcdd192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()  # root\n",
    "dataset_name = 'Full_Image'\n",
    "ckpt_path = os.path.join(current_path, 'ckpt')\n",
    "\n",
    "# 테스트용\n",
    "image_path = os.path.abspath(current_path + \"/../../../test\")\n",
    "images = os.listdir(image_path)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc59be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\working\\school\\CAPSTONE\\main\\model\\Full_Image\\original_data\\TG\\TIFF\n"
     ]
    }
   ],
   "source": [
    "# 테스트용\n",
    "image_path = os.path.abspath(current_path + \"/../../../school/CAPSTONE/main/model/Full_Image/original_data/TG/TIFF\")\n",
    "images = os.listdir(image_path)\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15818e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\working\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\System\\ckpt\n",
      "['c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\1.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\10.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\11.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\12.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\13.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\14.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\2.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\3.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\4.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\5.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\6.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\7.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\8.tif', 'c:\\\\working\\\\school\\\\CAPSTONE\\\\main\\\\model\\\\Full_Image\\\\original_data\\\\TG\\\\TIFF\\\\9.tif']\n",
      "c:\\working\\school\\CAPSTONE\\main\\model\\Full_Image\\original_data\\TG\\TIFF\n",
      "['1.tif', '10.tif', '11.tif', '12.tif', '13.tif', '14.tif', '2.tif', '3.tif', '4.tif', '5.tif', '6.tif', '7.tif', '8.tif', '9.tif']\n"
     ]
    }
   ],
   "source": [
    "image_pathes = [os.path.join(image_path, x) for x in images]  # 시스템에서는 입력으로 받음\n",
    "image_path = image_pathes[0].split('\\\\')[:-1]\n",
    "image_dir = '\\\\'.join(image_path)\n",
    "image_file = [image.split(\"\\\\\")[-1] for image in image_pathes]\n",
    "\n",
    "print(ckpt_path)\n",
    "print(image_pathes)\n",
    "print(image_dir)\n",
    "print(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e54dbe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\working\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\n"
     ]
    }
   ],
   "source": [
    "sample_path = os.path.abspath(current_path+\"/../results\")\n",
    "\n",
    "sample_crop = os.path.join(sample_path,'preprocessing','Crop')  # 자른 이미지\n",
    "sample_label = os.path.join(sample_path,'preprocessing','Label')  # 자른 라벨\n",
    "\n",
    "sample_prediction = os.path.join(sample_path,'prediction')  # 결과 이미지들\n",
    "sample_segmentation = os.path.join(sample_path,'segmentation')  # 결과 마스크들\n",
    "print(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9f1362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rebuild_dir(sample_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "122e197d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base_names</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_dir</th>\n",
       "      <th>autolabeling_dir</th>\n",
       "      <th>img_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\1.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\1_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\1_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\10.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\10_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\10_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>11.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\11.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\11_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\11_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>12.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\12.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\12_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\12_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>13.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\13.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\13_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\13_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>14.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\14.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\14_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\14_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>2.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\2.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\2_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\2_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>3.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\3.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\3_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\3_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>4.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\4.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\4_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\4_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>5.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\5.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\5_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\5_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>6.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\6.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\6_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\6_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7</td>\n",
       "      <td>7.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\7.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\7_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\7_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>8.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\8.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\8_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\8_result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9</td>\n",
       "      <td>9.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\test\\9.tif</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\9_crop</td>\n",
       "      <td>c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\9_result</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   base_names file_name                         file_dir  \\\n",
       "0           1     1.tif   c:\\Users\\yy\\Desktop\\test\\1.tif   \n",
       "1          10    10.tif  c:\\Users\\yy\\Desktop\\test\\10.tif   \n",
       "2          11    11.tif  c:\\Users\\yy\\Desktop\\test\\11.tif   \n",
       "3          12    12.tif  c:\\Users\\yy\\Desktop\\test\\12.tif   \n",
       "4          13    13.tif  c:\\Users\\yy\\Desktop\\test\\13.tif   \n",
       "5          14    14.tif  c:\\Users\\yy\\Desktop\\test\\14.tif   \n",
       "6           2     2.tif   c:\\Users\\yy\\Desktop\\test\\2.tif   \n",
       "7           3     3.tif   c:\\Users\\yy\\Desktop\\test\\3.tif   \n",
       "8           4     4.tif   c:\\Users\\yy\\Desktop\\test\\4.tif   \n",
       "9           5     5.tif   c:\\Users\\yy\\Desktop\\test\\5.tif   \n",
       "10          6     6.tif   c:\\Users\\yy\\Desktop\\test\\6.tif   \n",
       "11          7     7.tif   c:\\Users\\yy\\Desktop\\test\\7.tif   \n",
       "12          8     8.tif   c:\\Users\\yy\\Desktop\\test\\8.tif   \n",
       "13          9     9.tif   c:\\Users\\yy\\Desktop\\test\\9.tif   \n",
       "\n",
       "                                                                                                                    autolabeling_dir  \\\n",
       "0    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\1_crop   \n",
       "1   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\10_crop   \n",
       "2   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\11_crop   \n",
       "3   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\12_crop   \n",
       "4   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\13_crop   \n",
       "5   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\14_crop   \n",
       "6    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\2_crop   \n",
       "7    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\3_crop   \n",
       "8    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\4_crop   \n",
       "9    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\5_crop   \n",
       "10   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\6_crop   \n",
       "11   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\7_crop   \n",
       "12   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\8_crop   \n",
       "13   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\preprocessing\\Label\\9_crop   \n",
       "\n",
       "                                                                                                                      img_dir  \n",
       "0    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\1_result  \n",
       "1   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\10_result  \n",
       "2   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\11_result  \n",
       "3   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\12_result  \n",
       "4   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\13_result  \n",
       "5   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\14_result  \n",
       "6    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\2_result  \n",
       "7    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\3_result  \n",
       "8    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\4_result  \n",
       "9    c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\5_result  \n",
       "10   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\6_result  \n",
       "11   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\7_result  \n",
       "12   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\8_result  \n",
       "13   c:\\Users\\yy\\Desktop\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\results\\prediction\\9_result  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 5500)\n",
    "\n",
    "base_name = ['.'.join(x.split('.')[:-1]) for x in image_file]\n",
    "df = pd.DataFrame({\n",
    "    'base_names': base_name,\n",
    "    'file_name': image_file,\n",
    "    'file_dir': [os.path.join(image_dir, x) for x in image_file],\n",
    "    'autolabeling_dir': [os.path.join(sample_label, f'{x}_crop') for x in base_name],  # 라벨링 결과\n",
    "    'img_dir': [os.path.join(sample_prediction, f'{x}_result') for x in base_name]})  # 결과 데이터\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc0693c",
   "metadata": {},
   "source": [
    "### tile, model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc340e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 사진 수:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:16<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 1.tif\n",
      "Saved cropped image: 1680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36/36 [00:15<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 10.tif\n",
      "Saved cropped image: 1296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:30<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 11.tif\n",
      "Saved cropped image: 3570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:15<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 12.tif\n",
      "Saved cropped image: 1482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:13<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 13.tif\n",
      "Saved cropped image: 1365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:23<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 14.tif\n",
      "Saved cropped image: 2115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 2.tif\n",
      "Saved cropped image: 1935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:11<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 3.tif\n",
      "Saved cropped image: 1517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:17<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 4.tif\n",
      "Saved cropped image: 1634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52/52 [00:20<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 5.tif\n",
      "Saved cropped image: 2499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:13<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 6.tif\n",
      "Saved cropped image: 1365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:03<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 7.tif\n",
      "Saved cropped image: 272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:03<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 8.tif\n",
      "Saved cropped image: 342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35/35 [00:18<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일명: 9.tif\n",
      "Saved cropped image: 1462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rebuild_dir(sample_crop)\n",
    "cropping_image(df, sample_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7215536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "ENCODER = 'efficientnet-b2'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = ['cell']\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE = 8\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.UnetPlusPlus(\n",
    "    encoder_name=ENCODER,\n",
    "    encoder_weights=ENCODER_WEIGHTS,\n",
    "    classes=len(CLASSES),\n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eca463d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "test_dataset = InferenceDataset(\n",
    "    images_dir=image_dir,\n",
    "    images_files=image_file,\n",
    "    classes=CLASSES,\n",
    "    # augmentation=get_validation_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87a2daeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, # 한 번에 하나의 원본 이미지(의 타일들)를 처리\n",
    "    shuffle=False, \n",
    "    num_workers=0,  # num_workers/는 시스템의 CPU 코어 수를 활용할 수 있지만, 여기서는 0으로 설정하여 단일 프로세스에서 실행\n",
    "    # num_workers=os.cpu_count(), # 시스템의 CPU 코어 수 활용\n",
    "    collate_fn=inference_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8d0aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yy\\AppData\\Local\\Temp\\ipykernel_21912\\573539083.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model = torch.load(os.path.join(ckpt_path,'{}_model2.pth'.format(dataset_name)), map_location=DEVICE)\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load(os.path.join(ckpt_path,'{}_model2.pth'.format(dataset_name)), map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8b421",
   "metadata": {},
   "source": [
    "### blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ecbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    print(df.loc[i,'file_name'])\n",
    "    result_path = df.loc[i,'img_dir']\n",
    "    name = df.loc[i,'base_names']\n",
    "\n",
    "    blurrd_path = os.path.join(result_path, f'{name}_blurrd.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03d990",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69767775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress:   0%|          | 0/382 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference Progress:   5%|▌         | 21/382 [12:50<3:40:53, 36.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_masks\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 추론 실행\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m full_masks \u001b[38;5;241m=\u001b[39m \u001b[43minference_on_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[78], line 15\u001b[0m, in \u001b[0;36minference_on_folder\u001b[1;34m(model, dataloader, device, inference_dir)\u001b[0m\n\u001b[0;32m     12\u001b[0m tiles_batch \u001b[38;5;241m=\u001b[39m (tiles_batch)\u001b[38;5;241m.\u001b[39mto(device)  \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 모델 예측\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiles_batch\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# 출력: (batch_size, num_classes, H, W)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(image_names)):\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:29\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[1;32m---> 29\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[0;32m     32\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegmentation_head(decoder_output)\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\segmentation_models_pytorch\\encoders\\efficientnet.py:73\u001b[0m, in \u001b[0;36mEfficientNetEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     71\u001b[0m             drop_connect \u001b[38;5;241m=\u001b[39m drop_connect_rate \u001b[38;5;241m*\u001b[39m block_number \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocks)\n\u001b[0;32m     72\u001b[0m             block_number \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m---> 73\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_connect\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\efficientnet_pytorch\\model.py:109\u001b[0m, in \u001b[0;36mMBConvBlock.forward\u001b[1;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[0;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn0(x)\n\u001b[0;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swish(x)\n\u001b[1;32m--> 109\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_depthwise_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bn1(x)\n\u001b[0;32m    111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swish(x)\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yy\\Desktop\\school\\CAPSTONE\\submit\\hbnu-swuniv-capstone-project-come-capstone25-CAPSTONE-InnoMedics\\003Code\\myenv38\\lib\\site-packages\\efficientnet_pytorch\\utils.py:275\u001b[0m, in \u001b[0;36mConv2dStaticSamePadding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    274\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_padding(x)\n\u001b[1;32m--> 275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # 추론 시에는 그래디언트 계산을 비활성화하여 메모리 사용량을 줄이고 속도를 높입니다.\n",
    "def inference_on_folder(model, dataloader, device, inference_dir):\n",
    "    model.eval().to(device)\n",
    "    progress_bar = tqdm(dataloader, desc=\"Inference Progress\")\n",
    "    \n",
    "    full_masks = {}\n",
    "    num = 0\n",
    "    \n",
    "    for i, data in enumerate(progress_bar):\n",
    "        tiles_batch, h_starts, w_starts, heights, widths, image_names = data\n",
    "        #print(f\"\\n[{i+1}/{len(dataloader)}] 배치 추론 중...\")        \n",
    "        tiles_batch = (tiles_batch).to(device)  \n",
    "        \n",
    "        # 모델 예측\n",
    "        prediction = model(tiles_batch) # 출력: (batch_size, num_classes, H, W)\n",
    "        predictions = prediction.cpu().numpy() \n",
    "        \n",
    "        for j in range(len(image_names)):\n",
    "            img_name = image_names[j]\n",
    "            h_start = h_starts[j]\n",
    "            w_start = w_starts[j]\n",
    "            height = heights[j]\n",
    "            width = widths[j]\n",
    "            predicted_mask_tile = predictions[j]\n",
    "            # 활성화 함수 적용 후 CPU로 이동하고 NumPy 배열로 변환\n",
    "            # sigmoid 활성화 함수를 사용하면 [0, 1] 범위의 확률 맵이 출력됩니다.\n",
    "            # 'cell' 클래스 하나만 예측하므로 prediction.squeeze()를 사용합니다.\n",
    "            # threshold를 적용하여 이진 마스크로 변환 (0.5 이상이면 1, 아니면 0)\n",
    "            \n",
    "            # 단일 클래스 이진 세그멘테이션의 경우 (1, H, W) -> (H, W)\n",
    "            if len(CLASSES) == 1:\n",
    "                predicted_mask_tile = predicted_mask_tile.squeeze(0) # (H, W)\n",
    "                predicted_mask_tile = (predicted_mask_tile > 0.5).astype(np.uint8) * 255 # 이진 마스크로 변환 (0 또는 255)\n",
    "            else:\n",
    "                # 다중 클래스 세그멘테이션의 경우 argmax 사용\n",
    "                predicted_mask_tile = np.argmax(predicted_mask_tile, axis=0).astype(np.uint8) # (H, W)\n",
    "                # 클래스 ID를 실제 픽셀 값으로 매핑해야 할 수 있습니다. (예: 0, 1, 2... 에 따라 다른 색상)\n",
    "                # 여기서는 예시로 255로 스케일링하여 저장합니다.\n",
    "                predicted_mask_tile = predicted_mask_tile * (255 // (len(CLASSES) -1)) if len(CLASSES) > 1 else predicted_mask_tile * 255\n",
    "            tile_path = os.path.join(inference_dir,f\"{img_name}_crop\", f\"{num:05d}.png\")\n",
    "            cv2.imwrite(tile_path, predicted_mask_tile)     \n",
    "            \n",
    "            if has_less_white_pixels(tile_path):\n",
    "                os.remove(tile_path)\n",
    "                continue\n",
    "            num += 1\n",
    "\n",
    "            # --- 전체 마스크 재구성 로직 ---\n",
    "            # 현재 이미지에 대한 전체 마스크가 딕셔너리에 없으면 초기화\n",
    "            if img_name not in full_masks:\n",
    "                full_masks[img_name] = np.zeros((height, width), dtype=np.uint8)\n",
    "            \n",
    "            h_end = min(h_start + 512, height)\n",
    "            w_end = min(w_start + 512, width)\n",
    "            \n",
    "            # 재구성할 마스크의 실제 높이와 너비\n",
    "            actual_reconstruct_height = h_end - h_start\n",
    "            actual_reconstruct_width = w_end - w_start\n",
    "            \n",
    "            # 예측된 타일 마스크에서 원본 영역에 해당하는 부분만 추출\n",
    "            reconstructed_tile = predicted_mask_tile[:actual_reconstruct_height, :actual_reconstruct_width]\n",
    "            \n",
    "            # 전체 마스크에 재구성\n",
    "            full_masks[img_name][h_start:h_end, w_start:w_end] = reconstructed_tile\n",
    "        # 현재 배치의 추론 진행 상황 출력\n",
    "        #print(f\"[{i+1}/{len(dataloader)}] 배치 추론 완료.\")\n",
    "\n",
    "    # 모든 배치가 처리된 후, 최종 마스크들을 파일로 저장\n",
    "    print(\"\\n모든 이미지 타일 추론이 완료되었습니다. 최종 결과를 저장합니다. 💾\") \n",
    "    return full_masks\n",
    "    \n",
    "# 추론 실행\n",
    "full_masks = inference_on_folder(best_model, test_dataloader, DEVICE, sample_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for img_name,full_mask in (full_masks.items()):\n",
    "    image = cv2.imread(os.path.join(df.loc[idx, 'file_dir']))\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    output_dir = os.path.join(df.loc[idx, 'img_dir'], f\"{img_name}_contours.png\")\n",
    "    result_dir = os.path.join(sample_segmentation, f\"{img_name}_segmentation.png\")\n",
    "\n",
    "\n",
    "    blurrd_path = os.path.join(df.loc[idx, 'img_dir'], f'{img_name}_blurrd.png')\n",
    "    blurrd_image = cv2.imread(blurrd_path, cv2.IMREAD_COLOR)\n",
    "    image_with_prediction_contours = draw_contours_on_image(       image, full_mask.astype(np.uint8), (0, 255, 0))  # 초록색 prediction 테두리\n",
    "        \n",
    "    idx += 1\n",
    "    \n",
    "    cv2.imwrite(output_dir, image_with_prediction_contours)\n",
    "    cv2.imwrite(result_dir, full_mask)\n",
    "    print(f\"'{img_name}'의 예측 마스크를 '{output_dir}'에 저장했습니다. ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
